{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e76e2c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensordict import MemoryMappedTensor, TensorDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "69b240f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.0378, -0.2457],\n",
      "         [ 0.9079,  0.9024],\n",
      "         [ 0.5429, -1.0382],\n",
      "         [ 0.8087, -0.1141],\n",
      "         [-0.4617, -1.7037],\n",
      "         [-0.2021, -0.6088],\n",
      "         [-0.6994, -1.3875],\n",
      "         [-0.0610,  0.0864],\n",
      "         [ 0.6182, -1.9175],\n",
      "         [-0.5597, -0.6317],\n",
      "         [ 0.7547, -1.0077],\n",
      "         [ 0.4440, -0.0141],\n",
      "         [ 1.0847, -0.1549],\n",
      "         [-0.6506, -1.3555],\n",
      "         [-0.2390,  0.4642]]], grad_fn=<ViewBackward0>) tensor([[[-0.0869,  0.9546],\n",
      "         [ 1.4300,  0.3155],\n",
      "         [ 0.8465,  0.5613],\n",
      "         [ 0.2271,  0.8223],\n",
      "         [ 0.4929,  0.7103],\n",
      "         [ 0.1141,  0.8699],\n",
      "         [ 0.5938,  0.6678],\n",
      "         [-0.4756,  1.1184],\n",
      "         [ 0.1470,  0.8561],\n",
      "         [-1.1627,  1.4079],\n",
      "         [ 2.0324,  0.0617],\n",
      "         [-0.6266,  1.1820],\n",
      "         [ 1.4739,  0.2970],\n",
      "         [ 2.1132,  0.0276],\n",
      "         [ 2.2273, -0.0205]]], grad_fn=<ViewBackward0>) tensor([[[-0.4875, -0.8032],\n",
      "         [-0.5098, -0.4745],\n",
      "         [-0.5289, -0.1935],\n",
      "         [-0.5602,  0.2681],\n",
      "         [-0.5155, -0.3902],\n",
      "         [-0.5927,  0.7480],\n",
      "         [-0.5058, -0.5332],\n",
      "         [-0.5554,  0.1975],\n",
      "         [-0.5284, -0.2002],\n",
      "         [-0.5834,  0.6108],\n",
      "         [-0.4558, -1.2700],\n",
      "         [-0.5000, -0.6195],\n",
      "         [-0.5189, -0.3405],\n",
      "         [-0.5174, -0.3623],\n",
      "         [-0.5451,  0.0453]]], grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "state_emb = torch.nn.Linear(10, 2)(torch.randn(3,5,10).reshape(1,-1, 10))\n",
    "# return_emb = torch.nn.Linear(1, 2)(torch.randn(3,5,1).reshape(1,-1, 1))\n",
    "return_emb = torch.nn.Linear(1, 2)(torch.randn(3,5,1).reshape(1,-1, 1))\n",
    "action_emb = torch.nn.Linear(1, 2)(torch.randn(3,5,1).reshape(1,-1, 1))\n",
    "\n",
    "print(state_emb, return_emb, action_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a886379d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0378, -0.2457],\n",
       "         [-0.0869,  0.9546],\n",
       "         [-0.4875, -0.8032],\n",
       "         [ 0.9079,  0.9024],\n",
       "         [ 1.4300,  0.3155],\n",
       "         [-0.5098, -0.4745],\n",
       "         [ 0.5429, -1.0382],\n",
       "         [ 0.8465,  0.5613],\n",
       "         [-0.5289, -0.1935],\n",
       "         [ 0.8087, -0.1141],\n",
       "         [ 0.2271,  0.8223],\n",
       "         [-0.5602,  0.2681],\n",
       "         [-0.4617, -1.7037],\n",
       "         [ 0.4929,  0.7103],\n",
       "         [-0.5155, -0.3902]],\n",
       "\n",
       "        [[-0.2021, -0.6088],\n",
       "         [ 0.1141,  0.8699],\n",
       "         [-0.5927,  0.7480],\n",
       "         [-0.6994, -1.3875],\n",
       "         [ 0.5938,  0.6678],\n",
       "         [-0.5058, -0.5332],\n",
       "         [-0.0610,  0.0864],\n",
       "         [-0.4756,  1.1184],\n",
       "         [-0.5554,  0.1975],\n",
       "         [ 0.6182, -1.9175],\n",
       "         [ 0.1470,  0.8561],\n",
       "         [-0.5284, -0.2002],\n",
       "         [-0.5597, -0.6317],\n",
       "         [-1.1627,  1.4079],\n",
       "         [-0.5834,  0.6108]],\n",
       "\n",
       "        [[ 0.7547, -1.0077],\n",
       "         [ 2.0324,  0.0617],\n",
       "         [-0.4558, -1.2700],\n",
       "         [ 0.4440, -0.0141],\n",
       "         [-0.6266,  1.1820],\n",
       "         [-0.5000, -0.6195],\n",
       "         [ 1.0847, -0.1549],\n",
       "         [ 1.4739,  0.2970],\n",
       "         [-0.5189, -0.3405],\n",
       "         [-0.6506, -1.3555],\n",
       "         [ 2.1132,  0.0276],\n",
       "         [-0.5174, -0.3623],\n",
       "         [-0.2390,  0.4642],\n",
       "         [ 2.2273, -0.0205],\n",
       "         [-0.5451,  0.0453]]], grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = torch.stack((state_emb,return_emb,action_emb), dim=1).permute(0,2,1,3).reshape(3, 3*5, 2)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "92b9e86a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([15, 2])"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "6c33027b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([15])"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "5da1aab6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 15, 2])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.nn.LayerNorm(2)(res).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f547b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTransformer(nn.Module):\n",
    "    def __init__(self,\n",
    "                 state_dim=51,\n",
    "                 action_dim=1,\n",
    "                 max_context_length=48,\n",
    "                 max_ep_length=48,                 \n",
    "                 model_dim=128\n",
    "                 ):\n",
    "        super().__init__()\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.max_context_length = max_context_length\n",
    "        self.max_ep_length = max_ep_length\n",
    "        self.model_dim = model_dim\n",
    "\n",
    "\n",
    "        self.embed_timestep = nn.Embedding(self.max_ep_length, self.model_dim)\n",
    "        self.embed_return = torch.nn.Linear(1, self.model_dim)\n",
    "        self.embed_state = torch.nn.Linear(self.state_dim, self.model_dim)\n",
    "        self.embed_action = torch.nn.Linear(self.action_dim, self.model_dim)\n",
    "        self.embed_ln = nn.LayerNorm(self.model_dim)\n",
    "\n",
    "        self.predict_action = nn.Sequential(\n",
    "            nn.Linear(self.model_dim, self.action_dim),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        decoder_layer = nn.TransformerDecoderLayer(\n",
    "            d_model=self.model_dim,\n",
    "            nhead=8,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        self.transformer = nn.TransformerDecoder(decoder_layer=decoder_layer, num_layers=6)\n",
    "\n",
    "    def forward(self, states, actions, returns_to_go, timesteps, padding_mask=None):\n",
    "        batch_size, seq_length = states.shape[0], states.shape[1]\n",
    "\n",
    "        if padding_mask is None:\n",
    "            padding_mask = torch.ones((batch_size, seq_length), dtype=torch.float32)\n",
    "        \n",
    "        state_embeddings = self.embed_state(states)\n",
    "        action_embeddings = self.embed_action(actions)\n",
    "        returns_embeddings = self.embed_return(returns_to_go)\n",
    "        time_embeddings = self.embed_timestep(timesteps)\n",
    "\n",
    "        # time embeddings are treated similar to positional embeddings\n",
    "        state_embeddings = state_embeddings + time_embeddings\n",
    "        action_embeddings = action_embeddings + time_embeddings\n",
    "        returns_embeddings = returns_embeddings + time_embeddings\n",
    "\n",
    "        stacked_inputs = torch.stack(\n",
    "            (returns_embeddings, state_embeddings, action_embeddings), dim=1\n",
    "        ).permute(0, 2, 1, 3).reshape(batch_size, 3*seq_length, self.model_dim)\n",
    "        stacked_inputs = self.embed_ln(stacked_inputs)\n",
    "\n",
    "        dummy_memory = torch.zeros(1, seq_length, self.model_dim)\n",
    "\n",
    "        causal_mask = torch.triu(torch.full((3*seq_length, 3*seq_length), float('-inf')), diagonal=1)\n",
    "\n",
    "        stacked_padding_mask = torch.stack((padding_mask,padding_mask,padding_mask), dim=1).permute(0,2,1).reshape(batch_size,3*seq_length)\n",
    "\n",
    "        x = self.transformer(tgt=stacked_inputs,\n",
    "                             memory=dummy_memory, \n",
    "                             tgt_mask=causal_mask,\n",
    "                             tgt_key_padding_mask=stacked_padding_mask)\n",
    "        x = x.reshape(batch_size, seq_length, 3, self.model_dim).permute(0, 2, 1, 3)\n",
    "\n",
    "        return self.predict_action(x[:,1])\n",
    "\n",
    "    def get_action(self, states, actions, rtg, timesteps):\n",
    "\n",
    "        # Add batch dimension and reshape to [1, seq_len, state_dim] so input matches Transformer input format\n",
    "        states = states.reshape(1, -1, self.state_dim)\n",
    "        actions = actions.reshape(1, -1, self.action_dim)\n",
    "        rtg = rtg.reshape(1, -1, 1)\n",
    "        timesteps = timesteps.reshape(1, -1)\n",
    "\n",
    "        if self.max_context_length is not None:\n",
    "            states = states[:,-self.max_context_length:]\n",
    "            actions = actions[:,-self.max_context_length:]\n",
    "            rtg = rtg[:,-self.max_context_length:]\n",
    "            timesteps = timesteps[:,-self.max_context_length:]\n",
    "\n",
    "            # pad all tokens to sequence length\n",
    "            padding_mask = torch.cat([torch.zeros(self.max_context_length-states.shape[1], dtype=torch.float32), \n",
    "                                      torch.ones(states.shape[1],dtype=torch.long)]).reshape(1,-1)\n",
    "            states = torch.cat([torch.zeros((states.shape[0], self.max_context_length-states.shape[1], self.state_dim)), \n",
    "                                states], dim=1)\n",
    "            actions = torch.cat([torch.zeros((actions.shape[0], self.max_context_length - actions.shape[1], self.action_dim)),\n",
    "                                 actions], dim=1)\n",
    "            rtg = torch.cat([torch.zeros((rtg.shape[0], self.max_context_length-rtg.shape[1], 1)), \n",
    "                             rtg], dim=1)\n",
    "            timesteps = torch.cat([torch.zeros((timesteps.shape[0], self.max_context_length-timesteps.shape[1]), dtype=torch.long),\n",
    "                                   timesteps], dim=1)\n",
    "        else:\n",
    "            padding_mask = None\n",
    "\n",
    "        action_preds = self.forward(states, actions, rtg, timesteps, padding_mask)\n",
    "        \n",
    "        return action_preds[0,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3b0257c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "states = torch.randn(1, 48, 51)\n",
    "actions = torch.randn(1, 48, 1)\n",
    "returns_to_go = torch.randn(1, 48, 1)\n",
    "timesteps = torch.arange(48).reshape(1, 48)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "1ec22e08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.4032], grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DecisionTransformer().get_action(states, actions, returns_to_go, timesteps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c68e1cee",
   "metadata": {},
   "source": [
    "# Prepare Train Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "128d3cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_td_array = torch.load('../outputs/battery_optimization_solution_1.pt', weights_only=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd400c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for elem in train_td_array:\n",
    "    rtg =  torch.flip(torch.cumsum(torch.flip(elem['next', 'reward'], dims=[0]), dim=0), dims=[0])\n",
    "    rtg = torch.concat((rtg, torch.zeros(1,1)))\n",
    "    elem['return_to_go'] = rtg[0:48]\n",
    "    elem['next', 'return_to_go'] = rtg[1:49]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b43f4cae",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "decision-transformer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
